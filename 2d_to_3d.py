# -*- coding: utf-8 -*-
"""2d_to_3d.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lGySq9GxAoyNYYfo9TXgy3v8JtQSeAQ
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install nerfstudio --quiet

import numpy as np
from pathlib import Path

dataset_path = Path("/content/drive/MyDrive/buddha_dataset/buddha")
output_path = dataset_path / "transforms.json"

transforms = {"camera_angle_x": 0.6911112, "frames": []}
for img_path in sorted(dataset_path.glob("*.png")):
    base_name = img_path.stem.split(".")[0]
    p_file = dataset_path / f"{base_name}_P.txt"

    if not p_file.exists():
        print(f"Warning: no P.txt for {img_path.name}")
        continue

    # Read 3x4 matrix
    P = np.loadtxt(p_file)

    # Convert 3x4 → 4x4
    if P.shape == (3, 4):
        P4 = np.vstack([P, [0, 0, 0, 1]])
    else:
        print(f"Warning: {p_file.name} is not 3x4, skipping")
        continue

    transforms["frames"].append({
        "file_path": str(img_path.name),
        "transform_matrix": P4.tolist()
    })

with open(output_path, "w") as f:
    import json
    json.dump(transforms, f, indent=4)

print(f"transforms.json created with {len(transforms['frames'])} frames")

!ns-export poisson \
    --load-config /content/drive/MyDrive/buddha_nerf_output/buddha/nerfacto/2025-10-22_094139/config.yml \
    --output-dir /content/drive/MyDrive/buddha_nerf_output/buddha_mesh

import os
os.path.exists("/content/drive/MyDrive/buddha_nerf_output/buddha/nerfacto/2025-10-22_094139/config.yml")

"""# All images in images folder in buddha"""

!mkdir -p /content/drive/MyDrive/buddha_dataset/buddha/images
!mv /content/drive/MyDrive/buddha_dataset/buddha/*_c.png /content/drive/MyDrive/buddha_dataset/buddha/images/

"""# Training"""

import os
import shutil

dataset_dir = "/content/drive/MyDrive/buddha_dataset/buddha"
images_dir = os.path.join(dataset_dir, "images")

os.makedirs(images_dir, exist_ok=True)

for f in os.listdir(dataset_dir):
    if f.endswith("_c.png"):
        shutil.move(os.path.join(dataset_dir, f), images_dir)

import os

images_dir = "/content/drive/MyDrive/buddha_dataset/buddha/images"

for f in os.listdir(images_dir):
    if f.endswith("._c.png"):
        old_path = os.path.join(images_dir, f)
        new_name = f.replace("._c.png", "_c.png")
        new_path = os.path.join(images_dir, new_name)
        os.rename(old_path, new_path)
        print(f"Renamed {f} -> {new_name}")

!pip install torch==2.1.0+cu118 torchvision==0.16.1+cu118 torchaudio==2.1.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118
!pip install nerfstudio==1.1.5
!pip install imageio opencv-python-headless==4.10.0.84 h5py matplotlib ipywidgets

"""
# 23rd Oct"""

!pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu118
!pip install --quiet open3d trimesh plyfile scikit-image

from pathlib import Path
import os
from google.colab import drive
drive.mount('/content/drive')

ROOT_RGB = Path('/content/drive/MyDrive/buddha_result_new/rgb')
ROOT_DEPTH = Path('/content/drive/MyDrive/buddha_result_new/depths')
ROOT_PCD = Path('/content/drive/MyDrive/buddha_result_new/pointclouds')
INFER_IMAGES = Path('/content/drive/MyDrive/buddha_dataset/buddha/images')

OUT_DIR = Path('/content/buddha_out')
OUT_DIR.mkdir(parents=True, exist_ok=True)

print("RGB samples example:", list(ROOT_RGB.glob('*'))[:3])
print("Depth samples example:", list(ROOT_DEPTH.glob('*'))[:3])
print("Pointcloud samples example:", list(ROOT_PCD.glob('*'))[:3])

import numpy as np, os, time, math
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T
from torchvision.models import resnet18
import open3d as o3d
import trimesh
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)

import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import numpy as np
import os

class BuddhaDepthDataset(Dataset):
    def __init__(self, rgb_dir, depth_dir, transform=None):
        self.rgb_dir = rgb_dir
        self.depth_dir = depth_dir
        self.rgb_images = sorted([f for f in os.listdir(rgb_dir) if f.endswith('.png')])
        self.depth_images = sorted([f for f in os.listdir(depth_dir) if f.endswith('.png')])
        self.transform = transform

        assert len(self.rgb_images) == len(self.depth_images), "Mismatch between RGB and Depth images count"

        self.rgb_transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor(),
        ])
        self.depth_transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.rgb_images)

    def __getitem__(self, idx):
        rgb_path = os.path.join(self.rgb_dir, self.rgb_images[idx])
        depth_path = os.path.join(self.depth_dir, self.depth_images[idx])

        rgb = Image.open(rgb_path).convert("RGB")
        depth = Image.open(depth_path).convert("L")

        rgb = self.rgb_transform(rgb)
        depth = self.depth_transform(depth)

        return rgb, depth

class DepthNet(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        res = resnet18(pretrained=pretrained)

        self.conv1 = res.conv1
        self.bn1 = res.bn1
        self.relu = res.relu
        self.maxpool = res.maxpool
        self.layer1 = res.layer1
        self.layer2 = res.layer2
        self.layer3 = res.layer3
        self.layer4 = res.layer4

        self.conv1x1 = nn.Conv2d(512, 256, 1)
        self.up1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)
        self.up2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
        self.up3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)
        self.up4 = nn.ConvTranspose2d(32, 16, 4, 2, 1)
        self.out = nn.Conv2d(16, 1, 3, padding=1)
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None: nn.init.zeros_(m.bias)
    def forward(self, x):
        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)
        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)
        x = self.conv1x1(x)
        x = F.relu(self.up1(x)); x = F.relu(self.up2(x)); x = F.relu(self.up3(x)); x = F.relu(self.up4(x))
        x = self.out(x)
        x = F.softplus(x)
        return x

import numpy as np
def load_intrinsics_for_sample(name, W, H, intrinsics_dir=None):
    """
    If you have camera intrinsics saved as .npy per-sample, set intrinsics_dir to that folder.
    Otherwise approximate focal length heuristically.
    """
    if intrinsics_dir:
        p = Path(intrinsics_dir)/(name + '.npy')
        if p.exists():
            K = np.load(p)
            return K

    fx = fy = 0.8 * max(W,H)
    cx = W/2.0
    cy = H/2.0
    K = np.array([[fx, 0, cx],
                  [0, fy, cy],
                  [0,  0,  1]])
    return K

def depth_to_points_numpy(depth, K):
    H, W = depth.shape
    ys, xs = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')
    zs = depth
    fx = K[0,0]; fy = K[1,1]; cx = K[0,2]; cy = K[1,2]
    x_cam = (xs - cx) * zs / fx
    y_cam = (ys - cy) * zs / fy
    pts = np.stack([x_cam, y_cam, zs], axis=-1).reshape(-1,3)
    mask = (pts[:,2] > 1e-4) & (~np.isnan(pts[:,2]))
    pts = pts[mask]
    return pts

def points_to_mesh_open3d(points, poisson_depth=8, out_mesh_path=None):
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)

    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.05, max_nn=30))
    pcd.normalize_normals()
    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=poisson_depth)
    densities = np.asarray(densities)

    try:
        vtx_to_remove = densities < np.quantile(densities, 0.01)
        mesh.remove_vertices_by_mask(vtx_to_remove)
    except Exception as e:
        print("density cropping failed:", e)
    if out_mesh_path:
        o3d.io.write_triangle_mesh(out_mesh_path, mesh)
    return mesh

import os, torch, torch.nn as nn, torch.optim as optim
from tqdm import tqdm

def train(model, train_loader, val_loader=None, epochs=20, lr=1e-4, out_dir='/content/buddha_out', debug=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)
    criterion = nn.L1Loss()
    os.makedirs(out_dir, exist_ok=True)

    for epoch in range(1, epochs+1):
        model.train()
        running_loss = 0.0
        iters = 0
        loop = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}", ncols=120)
        for batch in loop:
           if isinstance(batch, dict):
                if 'rgb' in batch and 'depth' in batch:
                    imgs = batch['rgb']
                    depths = batch['depth']
                else:
                    keys = list(batch.keys())

                    imgs = batch[keys[0]]
                    depths = batch[keys[1]]
            elif isinstance(batch, (list, tuple)):
                imgs = batch[0]
                depths = batch[1]
            else:
                raise RuntimeError("Unsupported batch type: {}".format(type(batch)))

            imgs = imgs.to(device)
            depths = depths.to(device)

            preds = model(imgs)

            if preds.shape != depths.shape:
                depths_resized = torch.nn.functional.interpolate(depths, size=preds.shape[2:], mode='bilinear', align_corners=False)
            else:
                depths_resized = depths

            loss = criterion(preds, depths_resized)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            iters += 1

            if debug and (iters % 10 == 0):
                print(f"[DEBUG] batch shapes: imgs={tuple(imgs.shape)}, preds={tuple(preds.shape)}, depths={tuple(depths.shape)}")

            loop.set_postfix(train_loss = running_loss / (iters + 1e-12))

        avg_train_loss = running_loss / (iters + 1e-12)
        print(f"==> Epoch {epoch} finished. Avg train loss: {avg_train_loss:.6f}")

        if val_loader is not None:
            model.eval()
            val_loss = 0.0
            v_iters = 0
            with torch.no_grad():
                for batch in val_loader:
                    if isinstance(batch, dict):
                        imgs = batch['rgb'] if 'rgb' in batch else batch[list(batch.keys())[0]]
                        depths = batch['depth'] if 'depth' in batch else batch[list(batch.keys())[1]]
                    elif isinstance(batch, (list, tuple)):
                        imgs = batch[0]; depths = batch[1]
                    else:
                        raise RuntimeError("Unsupported batch type in val: {}".format(type(batch)))

                    imgs = imgs.to(device)
                    depths = depths.to(device)
                    preds = model(imgs)

                    if preds.shape != depths.shape:
                        depths_resized = torch.nn.functional.interpolate(depths, size=preds.shape[2:], mode='bilinear', align_corners=False)
                    else:
                        depths_resized = depths

                    val_loss += criterion(preds, depths_resized).item()
                    v_iters += 1
            avg_val_loss = val_loss / (v_iters + 1e-12)
            print(f"----> Validation loss: {avg_val_loss:.6f}")

            torch.save(model.state_dict(), os.path.join(out_dir, f'depth_epoch{epoch}.pth'))
        else:
            torch.save(model.state_dict(), os.path.join(out_dir, f'depth_epoch{epoch}.pth'))

    torch.save(model.state_dict(), os.path.join(out_dir, 'depth_model_final.pth'))
    print("Training complete. Final model saved to:", os.path.join(out_dir, 'depth_model_final.pth'))
    return model

IMG_SIZE = (256,256)   # H,W
BATCH_SIZE = 6
NUM_WORKERS = 2

names = [p.stem for p in sorted(ROOT_RGB.glob('*.png'))]  # uses rgb filenames

import random
random.shuffle(names)
split = int(0.8 * len(names))
train_names = names[:split]
val_names = names[split: split + max(1, int(0.1*len(names)))]

train_ds = BuddhaPairDataset(ROOT_RGB, ROOT_DEPTH, pcd_dir=ROOT_PCD, names_list=train_names, img_size=IMG_SIZE)
val_ds   = BuddhaPairDataset(ROOT_RGB, ROOT_DEPTH, pcd_dir=ROOT_PCD, names_list=val_names, img_size=IMG_SIZE)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

model = DepthNet(pretrained=True)
model = train(model, train_loader, val_loader, epochs=20, lr=1e-4, out_dir=str(OUT_DIR))

# load best model if exists
ckpt = OUT_DIR / 'best_depthnet.pth'
if ckpt.exists():
    model.load_state_dict(torch.load(str(ckpt), map_location=device))
    print("Loaded checkpoint:", ckpt)
model = model.to(device)
model.eval()

infer_img_path = INFER_IMAGES / '00002_c.png'

def infer_on_rgb_path(model, img_path, out_prefix='/content/buddha_out/result', target_size=(256,256)):
    img = Image.open(img_path).convert('RGB')
    W,H = target_size
    img_r = img.resize((W,H), Image.BILINEAR)
    transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])
    t = transform(img_r).unsqueeze(0).to(device)
    with torch.no_grad():
        pred = model(t).cpu().squeeze(0).squeeze(0).numpy()
    name = Path(img_path).stem
    K = load_intrinsics_for_sample(name, W, H, intrinsics_dir=None)
    pts = depth_to_points_numpy(pred, K)
    print("Predicted points:", pts.shape)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pts)
    pcd_path = f"{out_prefix}_{name}.ply"
    o3d.io.write_point_cloud(pcd_path, pcd)
    # Convert to mesh
    mesh_path = f"{out_prefix}_{name}_mesh.ply"
    mesh = points_to_mesh_open3d(pts, poisson_depth=8, out_mesh_path=mesh_path)
    # Convert ply -> gltf via trimesh
    tri = trimesh.load(mesh_path)
    gltf_path = f"{out_prefix}_{name}.gltf"
    tri.export(gltf_path)
    print("Saved:", pcd_path, mesh_path, gltf_path)
    return pcd_path, mesh_path, gltf_path

from pathlib import Path
pcd_path, mesh_path, gltf_path = infer_on_rgb_path(model, infer_img_path, out_prefix=str(OUT_DIR / 'result'))

# Visualize predicted depth and pointcloud preview
import matplotlib.pyplot as plt
# load ply and display basic info
print("Pointcloud path:", pcd_path)
pcd = o3d.io.read_point_cloud(pcd_path)
print("Num points:", np.asarray(pcd.points).shape)
# quick 2D depth preview (if you want to inspect pred)
# load pred as numpy if you saved it earlier; otherwise infer again and plot:
def show_pred(model, img_path, size=(256,256)):
    img = Image.open(img_path).convert('RGB').resize(size, Image.BILINEAR)
    t = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])(img).unsqueeze(0).to(device)
    with torch.no_grad():
        pred = model(t).cpu().squeeze().numpy()
    plt.figure(figsize=(8,4))
    plt.subplot(1,2,1); plt.imshow(np.array(img)); plt.title('input')
    plt.subplot(1,2,2); plt.imshow(pred, cmap='inferno'); plt.title('predicted depth'); plt.colorbar()
show_pred(model, infer_img_path)

import matplotlib.pyplot as plt
import torch
from torchvision.utils import make_grid

model.eval()

# Fetch a few test images
batch = next(iter(val_loader))
imgs = batch['rgb'].to('cuda')
depths = batch['depth'].to('cuda')

with torch.no_grad():
    preds = model(imgs)

# Resize GT depth if needed
if preds.shape != depths.shape:
    depths = torch.nn.functional.interpolate(depths, size=preds.shape[2:], mode='bilinear', align_corners=False)

# Convert to numpy for visualization
img_np = imgs.cpu().permute(0, 2, 3, 1).numpy()
depth_gt = depths.cpu().squeeze().numpy()
depth_pred = preds.cpu().squeeze().numpy()

# Show sample
i = 0
plt.figure(figsize=(15,5))
plt.subplot(1,3,1); plt.imshow(img_np[i]); plt.title("RGB Input"); plt.axis('off')
plt.subplot(1,3,2); plt.imshow(depth_gt[i], cmap='inferno'); plt.title("Ground Truth Depth"); plt.axis('off')
plt.subplot(1,3,3); plt.imshow(depth_pred[i], cmap='inferno'); plt.title("Predicted Depth"); plt.axis('off')
plt.show()

!pip install open3d -q
import open3d as o3d

# Load the point cloud
pcd_path = "/content/buddha_out/result_00002_c.ply"
pcd = o3d.io.read_point_cloud(pcd_path)

print(pcd)
o3d.visualization.draw_geometries([pcd], window_name="Raw Point Cloud")

# Estimate normals (required for mesh reconstruction)
pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))

# Poisson Surface Reconstruction
print("Running Poisson reconstruction (this may take 1–2 mins)...")
mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)

# Crop out low-density noise
bbox = pcd.get_axis_aligned_bounding_box()
mesh_crop = mesh.crop(bbox)

# Save the reconstructed mesh
mesh_path = "/content/buddha_out/buddha_mesh_poisson.ply"
o3d.io.write_triangle_mesh(mesh_path, mesh_crop)
print(f"Exported mesh to: {mesh_path}")

# Visualize the reconstructed mesh
o3d.visualization.draw_geometries([mesh_crop], window_name="Reconstructed Mesh")

mesh_glb = "/content/buddha_out/buddha_mesh.glb"
o3d.io.write_triangle_mesh(mesh_glb, mesh_crop, write_triangle_uvs=True)
print(f"Exported GLB: {mesh_glb}")

import open3d as o3d

mesh_path = "/content/buddha_out/buddha_mesh_poisson.ply"
mesh = o3d.io.read_triangle_mesh(mesh_path)

mesh.compute_vertex_normals()
mesh.orient_triangles()

mesh.remove_unreferenced_vertices()
mesh = mesh.filter_smooth_simple(number_of_iterations=3)
mesh = mesh.simplify_quadric_decimation(target_number_of_triangles=80000)

import numpy as np
import cv2

rgb_img = cv2.imread("/content/drive/MyDrive/buddha_result_new/rgb/00001._c.png")
rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
rgb_img = cv2.resize(rgb_img, (256,256))

vertices = np.asarray(mesh.vertices)
colors = []
for v in vertices:
    u = int(np.clip((v[0] - vertices[:,0].min()) / (np.ptp(vertices[:,0]) + 1e-6) * 255, 0, 255))
    v_coord = int(np.clip((v[1] - vertices[:,1].min()) / (np.ptp(vertices[:,1]) + 1e-6) * 255, 0, 255))

    colors.append(rgb_img[v_coord, u] / 255.0)
mesh.vertex_colors = o3d.utility.Vector3dVector(np.array(colors))

color_mesh_path = "/content/buddha_out/buddha_mesh_colored.ply"
o3d.io.write_triangle_mesh(color_mesh_path, mesh)
print(f"Saved colored mesh: {color_mesh_path}")

o3d.visualization.draw_geometries([mesh], window_name="Colored Mesh")

"""# Ending"""































"""# Fixed images"""

rgb_img = cv2.imread("/content/drive/MyDrive/buddha_result_new/rgb/00001._c.png")
if rgb_img is None:
    raise FileNotFoundError("Image not found. Check the path!")

rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
img_height, img_width, _ = rgb_img.shape

pcd = o3d.io.read_point_cloud("/content/drive/MyDrive/buddha_result_new/buddha_poisson.ply")
print("Number of points:", np.asarray(pcd.points).shape[0])

if not pcd.has_normals():
    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=1.0, max_nn=30))

vertices = np.asarray(pcd.points)

xyz_min = vertices.min(axis=0)
xyz_max = vertices.max(axis=0)
xyz_range = xyz_max - xyz_min + 1e-6

vertices_norm = (vertices - xyz_min) / xyz_range * 255

colors = []
for i, v in enumerate(vertices_norm):
    u = int(np.clip(v[0], 0, img_width-1))
    v_coord = int(np.clip(v[1], 0, img_height-1))
    colors.append(rgb_img[v_coord, u] / 255.0)

pcd.colors = o3d.utility.Vector3dVector(np.array(colors))

import cv2
rgb_img = cv2.imread("/content/drive/MyDrive/buddha_result_new/rgb/00001._c.png")
if rgb_img is None:
    raise FileNotFoundError("Image not found. Check the path!")

rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
img_height, img_width, _ = rgb_img.shape

colors = []
for i, v in enumerate(vertices_norm):
    u = int(np.clip(v[0], 0, img_width-1))
    v_coord = int(np.clip(v[1], 0, img_height-1))
    colors.append(rgb_img[v_coord, u] / 255.0)  # normalize to [0,1]

pcd.colors = o3d.utility.Vector3dVector(np.array(colors))

import open3d as o3d

o3d.visualization.draw_geometries([pcd])

o3d.io.write_point_cloud("/content/drive/MyDrive/buddha_result_new/buddha_colored.ply", pcd)

"""# Above was just points now with face too"""

import open3d as o3d
import numpy as np
import cv2

mesh = o3d.io.read_triangle_mesh("/content/drive/MyDrive/buddha_result_new/buddha_poisson.ply")
print(mesh)

rgb_img = cv2.imread("/content/drive/MyDrive/buddha_result_new/rgb/00001._c.png")
rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
img_height, img_width, _ = rgb_img.shape

vertices = np.asarray(mesh.vertices)

xyz_min = vertices.min(axis=0)
xyz_max = vertices.max(axis=0)
xyz_range = xyz_max - xyz_min + 1e-6
vertices_norm = (vertices - xyz_min) / xyz_range * 255

colors = []
for i, v in enumerate(vertices_norm):
    u = int(np.clip(v[0], 0, img_width-1))
    v_coord = int(np.clip(v[1], 0, img_height-1))
    colors.append(rgb_img[v_coord, u] / 255.0)

mesh.vertex_colors = o3d.utility.Vector3dVector(np.array(colors))

o3d.io.write_triangle_mesh("/content/drive/MyDrive/buddha_result_new/buddha_colored_mesh.ply", mesh)
o3d.io.write_triangle_mesh("/content/drive/MyDrive/buddha_result_new/buddha_colored_mesh.glb", mesh)

"""# 25th Oct
Now try to fix the output image
"""

import open3d as o3d
import numpy as np
import cv2

mesh = o3d.io.read_triangle_mesh("/content/drive/MyDrive/buddha_result_new/buddha_poisson.ply")
print(mesh)
mesh.compute_vertex_normals()

rgb_path = "/content/drive/MyDrive/buddha_result_new/rgb/00016._c.png"  # ensure correct name
rgb_img = cv2.imread(rgb_path)
if rgb_img is None:
    raise FileNotFoundError(f"Could not load image: {rgb_path}")
rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
h, w, _ = rgb_img.shape

verts = np.asarray(mesh.vertices)
verts -= verts.min(0)
verts /= verts.max(0)
verts[:, 0] *= (w - 1)
verts[:, 1] *= (h - 1)

verts[:, 1] = h - verts[:, 1]

vertex_colors = []
for v in verts:
    x, y = int(v[0]), int(v[1])
    if 0 <= x < w and 0 <= y < h:
        vertex_colors.append(rgb_img[y, x] / 255.0)
    else:
        vertex_colors.append([0.5, 0.5, 0.5])

mesh.vertex_colors = o3d.utility.Vector3dVector(np.array(vertex_colors))

R = mesh.get_rotation_matrix_from_xyz((np.pi / 2, 0, 0))
mesh.rotate(R, center=mesh.get_center())

o3d.io.write_triangle_mesh("/content/drive/MyDrive/buddha_result_new/buddha_final_colored.glb", mesh)
print("Exported: buddha_final_colored.glb")

"""# Using a monocular depth estimator (MiDaS)

"""

from google.colab import drive
drive.mount('/content/drive')

IMG_PATH = "/content/drive/MyDrive/buddha_result_new/rgb/00016._c.png"
OUT_DIR  = "/content/drive/MyDrive/buddha_result_new/"
import os
os.makedirs(OUT_DIR, exist_ok=True)

if not os.path.exists(IMG_PATH):
    raise FileNotFoundError(f"Image not found: {IMG_PATH}. Use !ls to inspect folder contents.")

import torch
import cv2
import numpy as np
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

img_bgr = cv2.imread(IMG_PATH)
if img_bgr is None:
    raise FileNotFoundError("Image not found or failed to load.")
img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

print("Loading MiDaS model (this will download weights first time)...")
midas = torch.hub.load("intel-isl/MiDaS", "DPT_Large")
midas.to(device).eval()

midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
transform = midas_transforms.dpt_transform

input_batch = transform(img).to(device)

with torch.no_grad():
    prediction = midas(input_batch)
    prediction = torch.nn.functional.interpolate(
        prediction.unsqueeze(1),
        size=img.shape[:2],
        mode="bicubic",
        align_corners=False,
    ).squeeze()

depth_map = prediction.cpu().numpy()

depth_min, depth_max = depth_map.min(), depth_map.max()
print("Depth min/max:", depth_min, depth_max)
depth_vis = (depth_map - depth_min) / (depth_max - depth_min)
plt.figure(figsize=(6,6)); plt.imshow(depth_vis, cmap='inferno'); plt.axis('off')
plt.show()

np.save(os.path.join(OUT_DIR, "depth_midas.npy"), depth_map)
cv2.imwrite(os.path.join(OUT_DIR, "depth_vis.png"), (depth_vis*255).astype(np.uint8)[:,:,None])

import open3d as o3d

h, w = depth_map.shape
focal_length = 0.8 * max(w, h)
cx, cy = w/2, h/2
fx = fy = focal_length

ys, xs = np.arange(h), np.arange(w)
grid_x, grid_y = np.meshgrid(xs, ys)

Z = depth_map
X = (grid_x - cx) * Z / fx
Y = (grid_y - cy) * Z / fy

Y = -Y

points = np.stack((X, Y, Z), axis=2).reshape(-1,3)
colors = img.reshape(-1,3) / 255.0

valid_mask = np.isfinite(points).all(axis=1) & (np.abs(points).sum(axis=1) < 1e3)
points = points[valid_mask]
colors = colors[valid_mask]

print("Points count before downsampling:", points.shape[0])

pcd = o3d.geometry.PointCloud()
pcd.points = o3d.utility.Vector3dVector(points)
pcd.colors = o3d.utility.Vector3dVector(colors)

pcd = pcd.voxel_down_sample(voxel_size=0.0015)
print("Points count after downsample:", np.asarray(pcd.points).shape[0])

pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.01, max_nn=30))
pcd.orient_normals_consistent_tangent_plane(30)

pcd_out = os.path.join(OUT_DIR, "buddha_from_depth.ply")
o3d.io.write_point_cloud(pcd_out, pcd)
print("Saved point cloud:", pcd_out)

print("Running Poisson reconstruction (may take time)...")
with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Warning) as cm:
    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)
print(mesh)

densities = np.asarray(densities)
density_threshold = np.quantile(densities, 0.02)
remove_mask = densities < density_threshold
mesh.remove_vertices_by_mask(remove_mask)

mesh.remove_degenerate_triangles()
mesh.remove_duplicated_vertices()
mesh.remove_duplicated_triangles()
mesh.remove_unreferenced_vertices()
mesh.compute_vertex_normals()

print("Mesh after cleaning:", mesh)
mesh_out_ply = os.path.join(OUT_DIR, "buddha_depth_mesh_cleaned.ply")
mesh_out_glb = os.path.join(OUT_DIR, "buddha_depth_mesh_cleaned.glb")
o3d.io.write_triangle_mesh(mesh_out_ply, mesh)
o3d.io.write_triangle_mesh(mesh_out_glb, mesh)
print("Saved mesh (PLY + GLB):", mesh_out_ply, mesh_out_glb)

import numpy as np
rgb = img.copy()
h, w, _ = rgb.shape

verts = np.asarray(mesh.vertices)
vmin = verts.min(axis=0)
vmax = verts.max(axis=0)
vrange = vmax - vmin + 1e-9
verts_norm = (verts - vmin) / vrange

xs = (verts_norm[:,0] * (w-1)).astype(int)
ys = ( (1.0 - verts_norm[:,1]) * (h-1) ).astype(int)

colors_vert = []
for xi, yi in zip(xs, ys):
    if 0 <= xi < w and 0 <= yi < h:
        colors_vert.append(rgb[yi, xi] / 255.0)
    else:
        colors_vert.append([0.5,0.5,0.5])

mesh.vertex_colors = o3d.utility.Vector3dVector(np.array(colors_vert))
o3d.io.write_triangle_mesh(mesh_out_glb, mesh)
print("Saved re-colored mesh:", mesh_out_glb)

triangle_clusters, cluster_n_triangles, cluster_area = mesh.cluster_connected_triangles()
cluster_n_triangles = np.asarray(cluster_n_triangles)
largest_cluster_idx = int(np.argmax(cluster_n_triangles))
tri_mask = np.asarray(triangle_clusters) == largest_cluster_idx
mesh.remove_triangles_by_mask(~tri_mask)
mesh.remove_unreferenced_vertices()
mesh.compute_vertex_normals()
o3d.io.write_triangle_mesh(os.path.join(OUT_DIR, "buddha_depth_mesh_largest.glb"), mesh)
print("Saved cleaned largest-component mesh.")

"""# 28th Oct"""

!pip install open3d trimesh torch torchvision plyfile scikit-image -q

from pathlib import Path
import torch, torchvision
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
import open3d as o3d
import trimesh, numpy as np, os, cv2
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

ROOT_RGB = Path('/content/drive/MyDrive/buddha_result_new/rgb')
OUT_DIR = Path('/content/drive/MyDrive/buddha_output')
OUT_DIR.mkdir(parents=True, exist_ok=True)

infer_img_path = ROOT_RGB / '00002._c.png'
print("Using input image:", infer_img_path)

class DepthNet(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        res = torchvision.models.resnet18(pretrained=pretrained)
        self.conv1 = res.conv1; self.bn1 = res.bn1; self.relu = res.relu; self.maxpool = res.maxpool
        self.layer1 = res.layer1; self.layer2 = res.layer2; self.layer3 = res.layer3; self.layer4 = res.layer4
        self.conv1x1 = nn.Conv2d(512, 256, 1)
        self.up1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)
        self.up2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
        self.up3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)
        self.up4 = nn.ConvTranspose2d(32, 16, 4, 2, 1)
        self.out = nn.Conv2d(16, 1, 3, padding=1)
    def forward(self, x):
        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)
        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)
        x = F.relu(self.up1(self.conv1x1(x)))
        x = F.relu(self.up2(x))
        x = F.relu(self.up3(x))
        x = F.relu(self.up4(x))
        x = self.out(x)
        return F.softplus(x)

def load_intrinsics_for_sample(W, H):
    fx = fy = 0.8 * max(W, H)
    cx, cy = W / 2, H / 2
    K = np.array([[fx, 0, cx],
                  [0, fy, cy],
                  [0, 0, 1]])
    return K

def depth_to_points_numpy(depth, K):
    H, W = depth.shape
    ys, xs = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')
    zs = depth
    fx, fy, cx, cy = K[0,0], K[1,1], K[0,2], K[1,2]
    x_cam = (xs - cx) * zs / fx
    y_cam = (ys - cy) * zs / fy
    pts = np.stack([x_cam, y_cam, zs], axis=-1).reshape(-1,3)
    pts = pts[(pts[:,2] > 1e-4) & (~np.isnan(pts[:,2]))]
    return pts

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DepthNet(pretrained=True).to(device).eval()

def infer_on_rgb_path(model, img_path, out_prefix):
    img = Image.open(img_path).convert("RGB")
    W, H = img.size
    img_r = img.resize((256,256))
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    ])
    t = transform(img_r).unsqueeze(0).to(device)

    with torch.no_grad():
        pred = model(t).cpu().squeeze().numpy()

    K = load_intrinsics_for_sample(W, H)
    pts = depth_to_points_numpy(pred, K)
    print("Predicted 3D points:", pts.shape)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pts)
    pcd_path = f"{out_prefix}.ply"
    o3d.io.write_point_cloud(pcd_path, pcd)
    print("Saved point cloud:", pcd_path)
    return pcd_path

pcd_path = infer_on_rgb_path(model, infer_img_path, str(OUT_DIR / 'result_00002._c'))

pcd = o3d.io.read_point_cloud(pcd_path)
print(f"Loaded {len(pcd.points)} points.")

if not pcd.has_normals():
    print("Estimating normals...")
    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))

print("Running Poisson reconstruction...")
mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)

bbox = pcd.get_axis_aligned_bounding_box()
mesh_crop = mesh.crop(bbox)

mesh_path = "/content/buddha_out/buddha_mesh_poisson.ply"
o3d.io.write_triangle_mesh(mesh_path, mesh_crop)
print(f"Mesh saved to: {mesh_path}")

rgb_img = cv2.imread(str(infer_img_path))
rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
rgb_img = cv2.resize(rgb_img, (256,256))

vertices = np.asarray(mesh_crop.vertices)
colors = []
for v in vertices:
    u = int(np.clip((v[0] - vertices[:,0].min()) / (np.ptp(vertices[:,0]) + 1e-6) * 255, 0, 255))
    v_coord = int(np.clip((v[1] - vertices[:,1].min()) / (np.ptp(vertices[:,1]) + 1e-6) * 255, 0, 255))
    colors.append(rgb_img[v_coord, u] / 255.0)
mesh_crop.vertex_colors = o3d.utility.Vector3dVector(np.array(colors))

color_mesh_path = "/content/drive/MyDrive/buddha_output/buddha_mesh_colored.ply"
o3d.io.write_triangle_mesh(color_mesh_path, mesh_crop)
print(f"Saved colored mesh: {color_mesh_path}")

try:
    o3d.visualization.draw_geometries([mesh_crop], window_name="Colored Mesh")
except:
    print("Colab headless mode — skipping visualization.")



from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os, torch, torch.nn as nn, torch.optim as optim

class BuddhaDepthDataset(Dataset):
    def __init__(self, rgb_dir, depth_dir):
        self.rgb_dir = rgb_dir
        self.depth_dir = depth_dir
        self.rgb_images = sorted([f for f in os.listdir(rgb_dir) if f.endswith('.png')])
        self.depth_images = sorted([f for f in os.listdir(depth_dir) if f.endswith('.png')])
        self.rgb_tf = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])
        self.depth_tf = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])
    def __len__(self): return len(self.rgb_images)
    def __getitem__(self, i):
        rgb = Image.open(os.path.join(self.rgb_dir, self.rgb_images[i])).convert('RGB')
        depth = Image.open(os.path.join(self.depth_dir, self.depth_images[i])).convert('L')
        return self.rgb_tf(rgb), self.depth_tf(depth)

train_ds = BuddhaDepthDataset("/content/drive/MyDrive/buddha_result_new/rgb",
                              "/content/drive/MyDrive/buddha_result_new/depths")
train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DepthNet(pretrained=True).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.L1Loss()

for epoch in range(10):
    model.train()
    total = 0
    for rgb, depth in train_loader:
        rgb, depth = rgb.to(device), depth.to(device)
        pred = model(rgb)

        if pred.shape != depth.shape:
          depth = torch.nn.functional.interpolate(depth, size=pred.shape[2:], mode='bilinear', align_corners=False)
        loss = criterion(pred, depth)

        optimizer.zero_grad(); loss.backward(); optimizer.step()
        total += loss.item()
    print(f"Epoch {epoch+1}: loss = {total/len(train_loader):.4f}")

torch.save(model.state_dict(), "/content/drive/MyDrive/buddha_output/depth_trained.pth")
print("Model trained and saved.")

from pathlib import Path
import torch
import open3d as o3d
import numpy as np
import cv2
from PIL import Image
import torchvision.transforms as T

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = DepthNet(pretrained=False).to(device)
model.load_state_dict(torch.load("/content/drive/MyDrive/buddha_output/depth_trained.pth", map_location=device))
model.eval()
print("Trained model loaded successfully.")

ROOT_RGB = Path("/content/drive/MyDrive/buddha_result_new/rgb")
OUT_DIR = Path("/content/drive/MyDrive/buddha_output")
OUT_DIR.mkdir(parents=True, exist_ok=True)

infer_img_path = ROOT_RGB / "00002._c.png"

def load_intrinsics_for_sample(W, H):
    fx = fy = 0.8 * max(W, H)
    cx, cy = W / 2, H / 2
    return np.array([[fx, 0, cx],
                     [0, fy, cy],
                     [0, 0, 1]])

def depth_to_points_numpy(depth, K):
    H, W = depth.shape
    ys, xs = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')
    zs = depth
    fx, fy, cx, cy = K[0,0], K[1,1], K[0,2], K[1,2]
    x_cam = (xs - cx) * zs / fx
    y_cam = (ys - cy) * zs / fy
    pts = np.stack([x_cam, y_cam, zs], axis=-1).reshape(-1,3)
    pts = pts[(pts[:,2] > 1e-4) & (~np.isnan(pts[:,2]))]
    return pts

def infer_on_rgb_path(model, img_path, out_prefix):
    img = Image.open(img_path).convert("RGB")
    W, H = img.size
    img_r = img.resize((256,256))
    transform = T.Compose([
        T.ToTensor(),
        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
    ])
    t = transform(img_r).unsqueeze(0).to(device)

    with torch.no_grad():
        pred = model(t).cpu().squeeze().numpy()

    K = load_intrinsics_for_sample(W, H)
    pts = depth_to_points_numpy(pred, K)
    print("Predicted 3D points:", pts.shape)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pts)
    pcd_path = f"{out_prefix}.ply"
    o3d.io.write_point_cloud(pcd_path, pcd)
    print("Saved point cloud:", pcd_path)
    return pcd_path

pcd_path = infer_on_rgb_path(model, infer_img_path, str(OUT_DIR / "result_00002_c"))

pcd = o3d.io.read_point_cloud(pcd_path)
print(f"Loaded {len(pcd.points)} points.")

if not pcd.has_normals():
    print("Estimating normals...")
    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))

pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)

print("Running Poisson reconstruction...")
mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)

bbox = pcd.get_axis_aligned_bounding_box()
mesh_crop = mesh.crop(bbox)

mesh_path = str(OUT_DIR / "buddha_mesh_poisson.ply")
o3d.io.write_triangle_mesh(mesh_path, mesh_crop)
print(f"Mesh saved to: {mesh_path}")

rgb_img = cv2.imread(str(infer_img_path))
rgb_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2RGB)
rgb_img = cv2.resize(rgb_img, (256,256))

vertices = np.asarray(mesh_crop.vertices)
colors = []
for v in vertices:
    u = int(np.clip((v[0] - vertices[:,0].min()) / (np.ptp(vertices[:,0]) + 1e-6) * 255, 0, 255))
    v_coord = int(np.clip((v[1] - vertices[:,1].min()) / (np.ptp(vertices[:,1]) + 1e-6) * 255, 0, 255))
    colors.append(rgb_img[v_coord, u] / 255.0)
mesh_crop.vertex_colors = o3d.utility.Vector3dVector(np.array(colors))

# Save colored mesh
color_mesh_path = str(OUT_DIR / "buddha_mesh_colored.ply")
o3d.io.write_triangle_mesh(color_mesh_path, mesh_crop)
print(f"Saved colored mesh: {color_mesh_path}")

mesh_glb = str(OUT_DIR / "buddha_mesh.glb")
o3d.io.write_triangle_mesh(mesh_glb, mesh_crop, write_triangle_uvs=True)
print(f"Exported GLB: {mesh_glb}")

!pip install -q open3d trimesh torchvision

from pathlib import Path
import os, time, math
import numpy as np
from PIL import Image
import cv2
import torch
import torchvision.transforms as T
import open3d as o3d
import torchvision
import torch.nn as nn
import torch.nn.functional as F

DRIVE_OUT = Path("/content/drive/MyDrive/buddha_output")
ROOT_RGB  = Path("/content/drive/MyDrive/buddha_result_new/rgb")
POSES_DIR = Path("/content/drive/MyDrive/buddha_result_new/poses")
MODEL_PATH = Path("/content/drive/MyDrive/buddha_output/depth_trained.pth")
IMG_RES = (256,256)
MAX_PER_IMAGE = 5000
CHUNK_MAX_POINTS = 250000
VOXEL_SIZE = 0.005
POISSON_DEPTH = 8
TMP_CHUNK_DIR = Path("/content/tmp_chunks")
TMP_CHUNK_DIR.mkdir(parents=True, exist_ok=True)
DRIVE_OUT.mkdir(parents=True, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

class DepthNet(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        res = torchvision.models.resnet18(pretrained=False)
        self.conv1 = res.conv1; self.bn1 = res.bn1; self.relu = res.relu; self.maxpool = res.maxpool
        self.layer1 = res.layer1; self.layer2 = res.layer2; self.layer3 = res.layer3; self.layer4 = res.layer4
        self.conv1x1 = nn.Conv2d(512, 256, 1)
        self.up1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)
        self.up2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
        self.up3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)
        self.up4 = nn.ConvTranspose2d(32, 16, 4, 2, 1)
        self.out = nn.Conv2d(16, 1, 3, padding=1)
    def forward(self, x):
        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)
        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)
        x = self.conv1x1(x)
        x = F.relu(self.up1(x)); x = F.relu(self.up2(x)); x = F.relu(self.up3(x)); x = F.relu(self.up4(x))
        x = self.out(x)
        return F.softplus(x)

model = DepthNet(pretrained=False).to(device)
if MODEL_PATH.exists():
    model.load_state_dict(torch.load(str(MODEL_PATH), map_location=device))
    print("Loaded trained model:", MODEL_PATH)
else:
    print("WARNING: trained model not found at", MODEL_PATH, "- using untrained model (results poor)")
model.eval()

transform = T.Compose([T.ToTensor(),
                       T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])

def intrinsics_for_image(w,h):
    fx = fy = 0.8 * max(w,h)
    cx, cy = w/2.0, h/2.0
    K = np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=np.float32)
    return K

def load_pose_for_image(image_path):
    stem = Path(image_path).stem
    candidates = [
        POSES_DIR / f"{stem}.txt",
        POSES_DIR / f"{stem}_pose.txt",
        POSES_DIR / f"{stem}.pose.txt",
        Path(image_path).with_suffix('.pose.txt'),
        Path(image_path).with_suffix('.txt'),
    ]
    for p in candidates:
        if p.exists():
            try:
                a = np.loadtxt(str(p))
                if a.size == 12:
                    a = a.reshape(3,4); T = np.vstack([a, [0,0,0,1]]); return T
                if a.size == 16:
                    return a.reshape(4,4)
            except Exception as e:
                print("Could not parse pose", p, e)
    return None

def predict_depth_for_image(img_path):
    img = Image.open(str(img_path)).convert('RGB')
    w,h = img.size
    img_resized = img.resize((IMG_RES[0], IMG_RES[1]), Image.BILINEAR)
    t = transform(img_resized).unsqueeze(0).to(device)
    with torch.no_grad():
        pred = model(t).cpu().squeeze(0).squeeze(0).numpy()
    pred_full = cv2.resize(pred, (w,h), interpolation=cv2.INTER_LINEAR)
    pred_full = np.maximum(pred_full, 1e-6).astype(np.float32)
    return pred_full, (w,h)

def depth_to_points_and_colors(depth, K, rgb_image, max_per_image=MAX_PER_IMAGE):
    H,W = depth.shape
    depth_flat = depth.reshape(-1)
    ys, xs = np.meshgrid(np.arange(H), np.arange(W), indexing='ij')
    xs_f = xs.reshape(-1).astype(np.float32)
    ys_f = ys.reshape(-1).astype(np.float32)
    mask = (np.isfinite(depth_flat)) & (depth_flat > 1e-6)
    if not np.any(mask):
        return np.zeros((0,3),dtype=np.float32), np.zeros((0,3),dtype=np.float32)
    depth_masked = depth_flat[mask]
    x_masked = xs_f[mask]; y_masked = ys_f[mask]
    fx, fy, cx, cy = K[0,0], K[1,1], K[0,2], K[1,2]
    x_cam = (x_masked - cx) * depth_masked / fx
    y_cam = (y_masked - cy) * depth_masked / fy
    pts = np.stack([x_cam, y_cam, depth_masked], axis=1)

    img = rgb_image
    if img.shape[0] != H or img.shape[1] != W:
        img = cv2.resize(img, (W,H))
    img_flat = img.reshape(-1,3)
    colors = img_flat[mask].astype(np.float32) / 255.0

    n = len(pts)
    if n > max_per_image:
        sel = np.random.choice(n, max_per_image, replace=False)
        pts = pts[sel]; colors = colors[sel]
    return pts.astype(np.float32), colors.astype(np.float32)

rgb_paths = sorted(list(ROOT_RGB.glob("*.png")) + list(ROOT_RGB.glob("*.jpg")))
if len(rgb_paths) == 0:
    raise RuntimeError(f"No images found in {ROOT_RGB}")

acc_pts = []
acc_colors = []
chunk_idx = 0
total_input_points = 0

print("Processing", len(rgb_paths), "images with per-image max", MAX_PER_IMAGE)
for i, p in enumerate(rgb_paths):
    try:
        depth, (w,h) = predict_depth_for_image(p)
    except Exception as e:
        print("Failed to predict depth for", p, e)
        continue

    K = intrinsics_for_image(w,h)
    rgb_img = cv2.cvtColor(cv2.imread(str(p)), cv2.COLOR_BGR2RGB)
    pts, colors = depth_to_points_and_colors(depth, K, rgb_img, max_per_image=MAX_PER_IMAGE)
    total_input_points += pts.shape[0]

    pose = load_pose_for_image(p)
    if pose is not None:
        homo = np.concatenate([pts, np.ones((pts.shape[0],1),dtype=np.float32)], axis=1)
        pts_world = (pose @ homo.T).T[:, :3]
    else:
        pts_world = pts

    acc_pts.append(pts_world)
    acc_colors.append(colors)

    current_count = sum([a.shape[0] for a in acc_pts])
    if current_count >= CHUNK_MAX_POINTS or (i+1) % 10 == 0:
        if current_count == 0:
            acc_pts, acc_colors = [], []
            continue
        chunk_idx += 1
        chunk_pc = o3d.geometry.PointCloud()
        chunk_pc.points = o3d.utility.Vector3dVector(np.concatenate(acc_pts, axis=0))
        chunk_pc.colors = o3d.utility.Vector3dVector(np.concatenate(acc_colors, axis=0))
        chunk_pc = chunk_pc.voxel_down_sample(voxel_size=VOXEL_SIZE)
        chunk_path = TMP_CHUNK_DIR / f"chunk_{chunk_idx:03d}.ply"
        o3d.io.write_point_cloud(str(chunk_path), chunk_pc)
        print(f" Flushed chunk {chunk_idx} -> {chunk_path} (points after voxel downsample: {len(chunk_pc.points)})")
        acc_pts, acc_colors = [], []

print("Total input points (before chunking/subsample):", total_input_points)

if len(acc_pts) > 0:
    chunk_idx += 1
    chunk_pc = o3d.geometry.PointCloud()
    chunk_pc.points = o3d.utility.Vector3dVector(np.concatenate(acc_pts, axis=0))
    chunk_pc.colors = o3d.utility.Vector3dVector(np.concatenate(acc_colors, axis=0))
    chunk_pc = chunk_pc.voxel_down_sample(voxel_size=VOXEL_SIZE)
    chunk_path = TMP_CHUNK_DIR / f"chunk_{chunk_idx:03d}.ply"
    o3d.io.write_point_cloud(str(chunk_path), chunk_pc)
    print(f" Final flush chunk {chunk_idx} -> {chunk_path} (points: {len(chunk_pc.points)})")
    acc_pts, acc_colors = [], []

chunk_files = sorted(TMP_CHUNK_DIR.glob("chunk_*.ply"))
if len(chunk_files) == 0:
    raise RuntimeError("No chunk files saved. Something went wrong in chunking stage.")
print("Merging", len(chunk_files), "chunks from disk...")

merged_pcd = o3d.geometry.PointCloud()
for cf in chunk_files:
    p = o3d.io.read_point_cloud(str(cf))
    merged_pcd += p

print("Merged points (before final voxel):", len(merged_pcd.points))
merged_pcd = merged_pcd.voxel_down_sample(voxel_size=VOXEL_SIZE)
print("After voxel_down_sample:", len(merged_pcd.points))

merged_pcd, ind = merged_pcd.remove_statistical_outlier(nb_neighbors=30, std_ratio=1.2)
print("After statistical outlier removal:", len(merged_pcd.points))

merged_pcd, ind2 = merged_pcd.remove_radius_outlier(nb_points=10, radius=VOXEL_SIZE*4.0)
print("After radius outlier removal:", len(merged_pcd.points))

pts_np = np.asarray(merged_pcd.points)
if len(pts_np) > 0:
    z_vals = pts_np[:,2]
    low_p, high_p = np.percentile(z_vals, [1, 99])
    mask = (z_vals >= low_p) & (z_vals <= high_p)
    idxs_keep = np.where(mask)[0].tolist()
    merged_pcd = merged_pcd.select_by_index(idxs_keep)
    print("After z-percentile trim:", len(merged_pcd.points))

center = merged_pcd.get_center()
merged_pcd.translate(-center)
print("Centered point cloud. Center:", center)

merged_path = DRIVE_OUT / "merged_pointcloud.ply"
o3d.io.write_point_cloud(str(merged_path), merged_pcd)
print("Saved merged point cloud to Drive:", merged_path)

if not merged_pcd.has_normals():
    merged_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=VOXEL_SIZE*5, max_nn=50))

print("Running Poisson reconstruction (depth=", POISSON_DEPTH, ") ...")
mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(merged_pcd, depth=POISSON_DEPTH)
dens = np.asarray(densities)

try:
    th = np.quantile(dens, 0.02)
    vtx_to_remove = dens < th
    mesh.remove_vertices_by_mask(vtx_to_remove)
    print("Filtered low-density vertices.")
except Exception as e:
    print("Vertex density filtering failed:", e)

mesh.remove_unreferenced_vertices()
mesh = mesh.filter_smooth_simple(number_of_iterations=2)
mesh = mesh.simplify_quadric_decimation(target_number_of_triangles=120000)
print("Mesh cleaned. Vertices:", len(mesh.vertices), "Triangles:", len(mesh.triangles))

pcd_tree = o3d.geometry.KDTreeFlann(merged_pcd)
merged_colors = np.asarray(merged_pcd.colors)
verts = np.asarray(mesh.vertices)
vertex_colors = np.zeros((len(verts),3), dtype=np.float32)
for vi, v in enumerate(verts):
    _, idx, _ = pcd_tree.search_knn_vector_3d(v, 1)
    if len(idx) > 0:
        vertex_colors[vi] = merged_colors[idx[0]]
    else:
        vertex_colors[vi] = np.array([0.8,0.8,0.8])
mesh.vertex_colors = o3d.utility.Vector3dVector(vertex_colors)

out_ply = DRIVE_OUT / "buddha_mesh_colored.ply"
out_glb = DRIVE_OUT / "buddha_mesh.glb"
o3d.io.write_triangle_mesh(str(out_ply), mesh, write_triangle_uvs=True)
print("Saved final PLY:", out_ply)
try:
    o3d.io.write_triangle_mesh(str(out_glb), mesh, write_triangle_uvs=True)
    print("Saved final GLB:", out_glb)
except Exception as e:
    print("GLB export failed (some Open3D builds). PLY saved. Error:", e)

print("ALL DONE. Files saved to:", DRIVE_OUT)
print("Temporary chunk files are in:", TMP_CHUNK_DIR)

"""# On Prosopo DATASET"""

!pip install -q kagglehub torch torchvision opencv-python open3d numpy matplotlib tqdm trimesh

import kagglehub, os, cv2, torch, numpy as np, open3d as o3d
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from tqdm import tqdm
from pathlib import Path

path = kagglehub.dataset_download("cantonioupao/prosopo-a-face-dataset-for-3d-reconstruction")
print("Dataset path:", path)

root = Path("/kaggle/input/prosopo-a-face-dataset-for-3d-reconstruction/prosopo_new")
rgb_dir = root / "input"
depth_dir = root / "depth128x128"
print("RGB folder:", rgb_dir)
print("Depth folder:", depth_dir)
class FaceDepthDataset(Dataset):
    def __init__(self, rgb_dir, depth_dir, transform=None):
        self.rgb_paths = sorted(list(rgb_dir.glob("*.jpg")) + list(rgb_dir.glob("*.png")) + list(rgb_dir.glob("*.jpeg")))
        self.depth_paths = sorted(list(depth_dir.glob("*.jpg")) + list(depth_dir.glob("*.png")) + list(depth_dir.glob("*.jpeg")))

        n = min(len(self.rgb_paths), len(self.depth_paths))
        self.rgb_paths = self.rgb_paths[:n]
        self.depth_paths = self.depth_paths[:n]

        print(f"Using {n} paired samples (RGB={len(self.rgb_paths)}, Depth={len(self.depth_paths)})")
        self.transform = transform


    def __len__(self):
        return len(self.rgb_paths)

    def __getitem__(self, idx):
        rgb = cv2.imread(str(self.rgb_paths[idx]))
        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)
        depth = cv2.imread(str(self.depth_paths[idx]), cv2.IMREAD_GRAYSCALE)
        rgb = cv2.resize(rgb, (128,128))
        depth = depth.astype(np.float32)/255.0
        if self.transform:
            rgb = self.transform(rgb)
        depth = torch.from_numpy(depth).unsqueeze(0)
        return rgb, depth

transform = transforms.Compose([
    transforms.ToTensor(),
])

dataset = FaceDepthDataset(rgb_dir, depth_dir, transform)
print("Total samples found:", len(dataset))
train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Device:", device)

class DepthNet(nn.Module):
    def __init__(self):
        super().__init__()
        base = models.resnet18(weights=None)
        self.encoder = nn.Sequential(*list(base.children())[:-2])
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 3, 2, 1, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 3, 2, 1, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 3, 2, 1, 1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 1, 3, 1, 1),
            nn.Sigmoid()
        )
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

model = DepthNet().to(device)
criterion = nn.L1Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

EPOCHS = 10
for epoch in range(EPOCHS):
    total_loss = 0
    for rgb, depth in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
        rgb, depth = rgb.to(device), depth.to(device)
        pred = model(rgb)
        pred = torch.nn.functional.interpolate(pred, size=depth.shape[2:], mode='bilinear', align_corners=False)
        loss = criterion(pred, depth)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: loss = {total_loss/len(train_loader):.4f}")

torch.save(model.state_dict(), "/content/depth_model_face.pth")
print("Model trained and saved")

def depth_to_pointcloud(depth, rgb):
    h, w = depth.shape
    fx = fy = 1.0
    cx, cy = w/2, h/2
    i, j = np.meshgrid(np.arange(w), np.arange(h))
    z = depth
    x = (i - cx) * z / fx
    y = (j - cy) * z / fy
    points = np.stack((x, -y, z), axis=-1).reshape(-1, 3)
    colors = rgb.reshape(-1, 3)/255.0
    return points, colors

os.makedirs("/content/output", exist_ok=True)
pcd_all = o3d.geometry.PointCloud()
test_imgs = (
    list(rgb_dir.glob("*.jpg")) +
    list(rgb_dir.glob("*.png")) +
    list(rgb_dir.glob("*.jpeg"))
)
print(f"Found {len(test_imgs)} test RGBs for reconstruction")

test_imgs = test_imgs[:20]

for img_path in tqdm(test_imgs, desc="Building point clouds"):
    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)
    inp = transform(cv2.resize(img, (128,128))).unsqueeze(0).to(device)
    with torch.no_grad():
        depth = model(inp).cpu().squeeze().numpy()
    depth = cv2.resize(depth, (img.shape[1], img.shape[0]))
    pts, colors = depth_to_pointcloud(depth, img)
    if len(pts) == 0:
        continue
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(pts)
    pcd.colors = o3d.utility.Vector3dVector(colors)
    pcd_all += pcd

print("Total points before cleaning:", len(pcd_all.points))

if len(pcd_all.points) == 0:
    raise RuntimeError("No points generated — check file extensions or paths")

pcd_all = pcd_all.voxel_down_sample(0.005)
pcd_all, _ = pcd_all.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
o3d.io.write_point_cloud("/content/output/merged_face_cloud.ply", pcd_all)
print("Pointcloud saved")

pcd_all.estimate_normals()
mesh, _ = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd_all, depth=8)
mesh.compute_vertex_normals()
o3d.io.write_triangle_mesh("/content/output/face_mesh.ply", mesh)
o3d.io.write_triangle_mesh("/content/output/face_mesh.glb", mesh)
print("Mesh saved successfully")

print("Vertices:", len(mesh.vertices), "Triangles:", len(mesh.triangles))
o3d.visualization.draw_geometries([mesh])

from google.colab import files
files.download("/content/output")

"""# using a pretrained MiDaS / DPT-Hybrid depth-estimation model from Intel"""

import os
from pathlib import Path
from PIL import Image

input_dir = "/content/input_images"
os.makedirs(input_dir, exist_ok=True)
img_path = Path(input_dir) / "sample_face.jpg"

!wget -q -O /content/input_images/sample_face.jpg https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=512

try:
    test_img = Image.open(img_path).convert("RGB")
    test_img.verify()
    print("✅ Image verified successfully:", img_path)
except Exception as e:
    print("❌ Still corrupted:", e)

import matplotlib.pyplot as plt
test_img = Image.open(img_path).convert("RGB")
plt.imshow(test_img)
plt.title("Verified Sample Face Input")
plt.axis("off")
plt.show()

import torch, numpy as np, cv2, open3d as o3d, matplotlib.pyplot as plt
from pathlib import Path
from PIL import Image
from transformers import DPTImageProcessor, DPTForDepthEstimation

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

model_id = "Intel/dpt-hybrid-midas"
processor = DPTImageProcessor.from_pretrained(model_id)
model = DPTForDepthEstimation.from_pretrained(model_id).to(device)
print("Loaded pretrained DPT-Hybrid model")

def predict_depth(image_path):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        depth = outputs.predicted_depth
    depth = torch.nn.functional.interpolate(
        depth.unsqueeze(1),
        size=image.size[::-1],
        mode="bicubic",
        align_corners=False,
    ).squeeze().cpu().numpy()

    depth = (depth - np.min(depth)) / (np.max(depth) - np.min(depth))
    return np.array(image), depth

def depth_to_pointcloud(rgb, depth, scale=1.0):
    h, w = depth.shape
    i, j = np.meshgrid(np.arange(w), np.arange(h))
    z = depth * scale
    x = (i - w / 2) * z / w
    y = (j - h / 2) * z / h

    points = np.stack((x, -y, z), axis=-1).reshape(-1, 3)
    colors = (rgb.reshape(-1, 3) / 255.0)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(points)
    pcd.colors = o3d.utility.Vector3dVector(colors)
    return pcd

img_path = "/content/input_images/sample_face.jpg"

rgb, depth = predict_depth(img_path)
plt.figure(figsize=(12,5))
plt.subplot(1,2,1); plt.imshow(rgb); plt.title("Input RGB"); plt.axis("off")
plt.subplot(1,2,2); plt.imshow(depth, cmap='inferno'); plt.title("Predicted Depth"); plt.axis("off")
plt.show()

pcd = depth_to_pointcloud(rgb, depth, scale=1.0)
pcd = pcd.voxel_down_sample(voxel_size=0.002)
pcd.estimate_normals()

mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=8)
vertices_to_remove = densities < np.quantile(densities, 0.02)
mesh.remove_vertices_by_mask(vertices_to_remove)
mesh.compute_vertex_normals()

out_dir = Path("/content/output")
out_dir.mkdir(exist_ok=True)
ply_path = out_dir / "reconstructed_face.ply"
glb_path = out_dir / "reconstructed_face.glb"

o3d.io.write_triangle_mesh(str(ply_path), mesh)
o3d.io.write_triangle_mesh(str(glb_path), mesh)
print(f"Saved reconstructed 3D mesh:\nPLY: {ply_path}\nGLB: {glb_path}")
o3d.visualization.draw_geometries([mesh])

